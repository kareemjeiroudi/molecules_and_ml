{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Notes\n",
    "\n",
    "Please do not run the file as whole, as some cells include some irrelevant code that was created for testing some examples. The cells to be ignored are noted by a **BEGIN:** and **END;** notations.\n",
    "\n",
    "First, let's go through the imports we'll need for this session. Iâ€™ll need RDkit for molecular conversion and fingerprints calculation, numpy for and data management and numpy arrays, Scikit-learn for standardisation and data set splitting as well as Keras for the neural network building and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data\n",
    "\n",
    "Reading a set of molecules an `SDMolSupplier` from rdkit.<br>\n",
    "In the [getting started](https://www.rdkit.org/docs/GettingStartedInPython.html) documentation there're two these two different suppliers:\n",
    "* `rdkit.Chem.rdmolfiles.SDMolSupplier`\n",
    "* `rdkit.Chem.rdmolfiles.SmilesMolSupplier`\n",
    "\n",
    "We're gonna be using the `SDMolSupplier`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the file\n",
    "supplier = Chem.SDMolSupplier('data/cas_4337.sdf')\n",
    "len(supplier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4337 molecules in the data set:\n",
    "A good practice is to test each molecule to see if it was correctly read before working with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mol in supplier:\n",
    "    if mol is None:\n",
    "        print(\"a None molecule was found!\")\n",
    "        \n",
    "## if the output is empty the data is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Morgan fingerprints\n",
    "\n",
    "We're going to go trough two steps:<br>\n",
    "\n",
    "First, we'll obtain the training samples, which are going to be the bits value returned by the function `AllChem.GetMorganFingerprintAsBitVect`.<br>This is typically the output of calculating the morgan fingerprints for each molecule. Please refer to the subsection '**Explaining bits from Morgan Fingerprints**' to understand the output.<br>\n",
    "After that we'd only need to turn it to a numpy array, so that we can pass it to Keras's sequential model. Keras's model expects the training set to be either a numpy array (if the model has a single input), or list of Numpy arrays (if the model has multiple inputs).\n",
    "\n",
    "## Training Samples\n",
    "\n",
    "> **\\_\\_TASK\\_\\_:** for each molecule calculate MorganFingerprints (with radius <b>3</b>) and size **~2048** (rdkit has also a nice easy function for that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rdkit.Chem import AllChem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4337, 2048)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = {} # will be mutated in the next function\n",
    "## calculate the Morgan Fingerprints for every molecule in the supplier\n",
    "fingerprints = [AllChem.GetMorganFingerprintAsBitVect(mol, 3, nBits=2048, bitInfo=info) for mol in supplier]\n",
    "## convert it from bit vector to NumPy array\n",
    "training_samples = np.array(fingerprints)\n",
    "\n",
    "training_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `bitsInfo` - Explaining bits from Morgan Fingerprints\n",
    "\n",
    "> **\\_\\_TASK\\_\\_:** Important! when you calculate the Fingerprints, save which atoms where responsible for the activation of the fingerprint (rdkit can also do that)\n",
    "\n",
    "Information about which atoms are contributing to the activation of the fingerprint is stored in the dictionary `bitsInfo`. The dictionary provided is populated with one entry per bit set in the fingerprint, <u>the keys are the bit ids</u>, <u>the values are lists of (atom index, radius) tuples</u>. Let's have a quick look on the dictionary in `info`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{97: ((14, 0),),\n",
       " 191: ((10, 1),),\n",
       " 263: ((4, 2),),\n",
       " 314: ((7, 1), (9, 1)),\n",
       " 325: ((15, 3),),\n",
       " 336: ((2, 2), (3, 2)),\n",
       " 389: ((17, 2),),\n",
       " 484: ((6, 3), (8, 3)),\n",
       " 606: ((11, 2),),\n",
       " 650: ((5, 0), (7, 0), (9, 0)),\n",
       " 689: ((1, 1),),\n",
       " 703: ((12, 2), (13, 2)),\n",
       " 807: ((1, 0),),\n",
       " 811: ((6, 2), (8, 2)),\n",
       " 856: ((11, 3),),\n",
       " 905: ((0, 3),),\n",
       " 993: ((0, 2),),\n",
       " 1019: ((0, 0),),\n",
       " 1034: ((2, 1), (3, 1)),\n",
       " 1060: ((6, 1), (8, 1)),\n",
       " 1077: ((10, 2),),\n",
       " 1088: ((15, 1), (16, 1), (17, 1)),\n",
       " 1114: ((6, 0), (8, 0)),\n",
       " 1152: ((4, 0),),\n",
       " 1199: ((15, 2), (16, 2)),\n",
       " 1216: ((1, 3),),\n",
       " 1327: ((10, 3),),\n",
       " 1380: ((2, 0), (3, 0), (10, 0), (11, 0)),\n",
       " 1460: ((12, 3),),\n",
       " 1642: ((4, 3),),\n",
       " 1645: ((11, 1),),\n",
       " 1682: ((1, 2),),\n",
       " 1717: ((14, 1),),\n",
       " 1750: ((12, 1), (13, 1)),\n",
       " 1771: ((0, 1),),\n",
       " 1816: ((4, 1),),\n",
       " 1873: ((12, 0), (13, 0), (15, 0), (16, 0), (17, 0)),\n",
       " 1917: ((5, 1),),\n",
       " 1947: ((17, 3),),\n",
       " 2037: ((2, 3),)}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Interpreting the results above:\n",
    "\n",
    "```\n",
    "97: ((14, 0),),\n",
    "191: ((10, 1),),\n",
    "314: ((7, 1), (9, 1)),\n",
    "```\n",
    "\n",
    "* bit 97 is set once: by atom 14 with radius 0.\n",
    "* bit 191 is set also once: by atom 10, with radius 1.\n",
    "* bit 314, on the other hand, is set twice: once by atom 7 and once with atom 9, each with radius 1.\n",
    "\n",
    "<br>\n",
    "\n",
    "And there we go. Now have succefully extracted the training samples that are ready to be passed to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "*The following code can be ignored, cause this was generated for my own interest and testing.*\n",
    "\n",
    "---\n",
    "BEGIN:\n",
    "\n",
    "In the following I'd like to draw one molecule in order to locate the Morgan fingerprint, together with the atoms and radius. So let's draw the molecule first, then let's find out what the following values correspond to:<br>\n",
    "The first entry in the `info` dictionary is `97: ((14, 0),),`, so we'll see what that means.<br>\n",
    "Lastly, I'll repeat the same process but with molecule number 314 whose fingerprints are `314: ((7, 1), (9, 1)),`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Draw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END;\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Labels\n",
    "\n",
    "Now that we've got the the bits ready as training samples (i.e. features) for the model, we still need to extract the label to have a target for our predictions.\n",
    "\n",
    "The property that we want to extract out of the list of prooperties, is whether or not the 'Ames test Categorisation' is a mutagen.\n",
    "* 1 for mutagen \n",
    "* 0 for nonmutagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = []\n",
    "for mol in supplier:\n",
    "    if mol.GetProp(\"Ames test categorisation\") == \"mutagen\":\n",
    "        training_labels.append(1)\n",
    "    else:\n",
    "        training_labels.append(0)\n",
    "\n",
    "training_labels = np.array(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4337"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we've got the whole training set ready. Let's move on to extracting the validation set.\n",
    "\n",
    "#### Validation Set\n",
    "\n",
    "In order to select a 'valid' validation, we have randomly picked the following indices of molecules.<br>\n",
    "Additionally, we'd still need to see how many fingerprints are expressed in this set of molecules.\n",
    "\n",
    "In the following, I'll be using the alias `val_`, for anything that's related to the validation set, in order to avoid length variables names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KAREEM: these molecules I got from Kristina!\n",
    "val_mol_ids = [6,   10,   29,   32,   42,   58,   72,   83,   98,  100,  128, \n",
    "        145,  148,  168,  171,  205,  208,  237,  244,  285,  290,  291,\n",
    "         300,  312,  332,  334,  335,  347,  356,  369,  371,  377,  407,\n",
    "         424,  456,  458,  470,  472,  486,  514,  515,  528,  557,  563,\n",
    "         599,  610,  616,  628,  640,  701,  704,  722,  764,  794,  818,\n",
    "         821,  840,  850,  856,  859,  874,  878,  882,  898,  901,  925,\n",
    "         936,  945,  957,  974,  977, 1013, 1019, 1030, 1038, 1047, 1049,\n",
    "        1072, 1073, 1100, 1159, 1168, 1187, 1190, 1194, 1201, 1202, 1233,\n",
    "        1247, 1258, 1264, 1273, 1283, 1288, 1300, 1302, 1319, 1339, 1349,\n",
    "        1402, 1413, 1416, 1422, 1426, 1435, 1454, 1465, 1483, 1502, 1513,\n",
    "        1515, 1520, 1548, 1576, 1604, 1606, 1621, 1650, 1695, 1696, 1711,\n",
    "        1714, 1716, 1725, 1743, 1746, 1752, 1780, 1788, 1794, 1799, 1813,\n",
    "        1826, 1866, 1886, 1901, 1903, 1921, 1929, 1940, 1969, 1970, 1997,\n",
    "        1998, 2008, 2010, 2011, 2018, 2023, 2046, 2060, 2064, 2080, 2081,\n",
    "        2131, 2171, 2182, 2203, 2212, 2224, 2231, 2241, 2246, 2283, 2294,\n",
    "        2295, 2297, 2327, 2329, 2331, 2349, 2357, 2360, 2365, 2397, 2413,\n",
    "        2417, 2418, 2421, 2448, 2467, 2510, 2516, 2528, 2533, 2549, 2562,\n",
    "        2601, 2604, 2606, 2609, 2611, 2632, 2644, 2653, 2677, 2682, 2685,\n",
    "        2692, 2703, 2708, 2714, 2719, 2726, 2732, 2759, 2761, 2776, 2780,\n",
    "        2817, 2818, 2829, 2837, 2857, 2858, 2884, 2899, 2902, 2905, 2911,\n",
    "        2939, 2975, 2977, 2986, 3007, 3009, 3018, 3024, 3038, 3066, 3087,\n",
    "        3098, 3107, 3117, 3122, 3139, 3157, 3161, 3164, 3217, 3223, 3233,\n",
    "        3263, 3265, 3271, 3290, 3295, 3307, 3313, 3317, 3321, 3382, 3384,\n",
    "        3388, 3400, 3409, 3412, 3419, 3423, 3449, 3470, 3487, 3488, 3503,\n",
    "        3509, 3511, 3539, 3562, 3626, 3637, 3654, 3662, 3663, 3668, 3671,\n",
    "        3688, 3689, 3695, 3710, 3726, 3743, 3744, 3782, 3791, 3794, 3808,\n",
    "        3809, 3841, 3849, 3874, 3910, 3912, 3925, 3945, 3950, 3958, 3959,\n",
    "        3962, 3964, 3967, 3978, 3993, 4009, 4010, 4055, 4057, 4085, 4089,\n",
    "        4096, 4099, 4107, 4112, 4129, 4135, 4151, 4155, 4196, 4209, 4216,\n",
    "        4234, 4236, 4251, 4267, 4283, 4317, 4326, 4335\n",
    "\n",
    "]\n",
    "val_molecules = [supplier[i] for i in val_mol_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_molecules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_info = {}\n",
    "val_fingerprints = [AllChem.GetMorganFingerprintAsBitVect(mol, 3, nBits=2048, bitInfo=val_info) for mol in val_molecules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = []\n",
    "for mol in val_molecules:\n",
    "    if mol.GetProp(\"Ames test categorisation\") == \"mutagen\":\n",
    "        val_labels.append(1)\n",
    "    else:\n",
    "        val_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an insignificant change of the data. The atempt here is create a tuples of both validation fingerprints and label put together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(327, 2)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# val_set = [tuple(fingerprints[i], val_labels[i]) for i range(len(fingerprints))]\n",
    "val_set = []\n",
    "for i in range(len(val_fingerprints)):\n",
    "    val_set.append((val_fingerprints[i], val_labels[i]))\n",
    "    \n",
    "val_set = np.array(val_set)\n",
    "val_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## here I'll try to recreate the vlidation set, I think it's a lot easier than I thought\n",
    "val_set = (val_mol_ids, val_labels)\n",
    "len(val_set)\n",
    "# val_set = [(val_mol_ids[i], val_labels[i]) for i in range(len(val_labels))]\n",
    "# val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "## Creating The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to suppress the FutureWarning: conversion of the second argument of issubdtype \n",
    "# from 'float' to 'np.floatin' is deprecated\n",
    "import os\n",
    "\n",
    "# importing all libraries that we'd need\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score # Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From before we know that our data shape is `(4337, 2048)`\n",
    "\n",
    "Standard scaling is applied now, because the model cannot learn on som arbitrary numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#Scale fingerprints to unit variance and zero mean\n",
    "st = StandardScaler()\n",
    "scaled_fingerprints= st.fit_transform(fingerprints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, I'd like to scale down the validation fingerprints too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_val_fingerprints = st.fit_transform(val_fingerprints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer must get an input dimensions matching the data $=2048$, whereas the following can deduce their input size from the previous layer. For instance, the output size of the firs layer in my model is 5, beginning with the second layer, I don't need to specify the input size, because the layer can deduce that alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=2048, units=5)`\n",
      "  \n",
      "/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1)`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(output_dim=5, input_dim=scaled_fingerprints.shape[1]))\n",
    "## Kristina wouldn't use sigmoid, either relu or selu \n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=1))\n",
    "model.add(Activation(\"linear\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I used the fasted learning rate possible because running 500 epochs takes relatively long time $(\\approx1.5\\,min)$ on my 2017 machine. For optimizers, I used standard SGD, which <...>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 5)                 10245     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 6         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 10,251\n",
      "Trainable params: 10,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for us to predict the <>, we used the function `sklearn.metrics.roc_auc_score`, which computes Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "Note: this implementation is restricted to the binary classification task or multilabel classification task in label indicator format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommendation of Kristina:<br>\n",
    "Use one `fit` and one `predict` at a time and instead of running multiple epochs, you can use a for loop in order for you validate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 1s 137us/step - loss: 0.0171 - acc: 0.9767\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 86us/step - loss: 0.0180 - acc: 0.9763\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 85us/step - loss: 0.0179 - acc: 0.9767\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 85us/step - loss: 0.0178 - acc: 0.9760\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 86us/step - loss: 0.0173 - acc: 0.9769\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 86us/step - loss: 0.0171 - acc: 0.9769\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 86us/step - loss: 0.0169 - acc: 0.9767\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 85us/step - loss: 0.0169 - acc: 0.9769\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 86us/step - loss: 0.0170 - acc: 0.9769\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 86us/step - loss: 0.0167 - acc: 0.9769\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 86us/step - loss: 0.0166 - acc: 0.9769\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 85us/step - loss: 0.0166 - acc: 0.9772\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 86us/step - loss: 0.0165 - acc: 0.9772\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 86us/step - loss: 0.0166 - acc: 0.9774\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 86us/step - loss: 0.0165 - acc: 0.9774\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 86us/step - loss: 0.0163 - acc: 0.9774\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 86us/step - loss: 0.0166 - acc: 0.9776\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 86us/step - loss: 0.0165 - acc: 0.9776\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 86us/step - loss: 0.0163 - acc: 0.9779\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 86us/step - loss: 0.0163 - acc: 0.9776\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 86us/step - loss: 0.0165 - acc: 0.9774\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 85us/step - loss: 0.0173 - acc: 0.9774\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 86us/step - loss: 0.0166 - acc: 0.9776\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 86us/step - loss: 0.0164 - acc: 0.9776\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 86us/step - loss: 0.0165 - acc: 0.9779\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 94us/step - loss: 0.0163 - acc: 0.9779\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 92us/step - loss: 0.0161 - acc: 0.9779\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 92us/step - loss: 0.0163 - acc: 0.9779\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 89us/step - loss: 0.0164 - acc: 0.9774\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 92us/step - loss: 0.0164 - acc: 0.9774\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 109us/step - loss: 0.0163 - acc: 0.9774\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 99us/step - loss: 0.0163 - acc: 0.9774\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 93us/step - loss: 0.0164 - acc: 0.9776\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 93us/step - loss: 0.0166 - acc: 0.9774\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 98us/step - loss: 0.0168 - acc: 0.9772\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 91us/step - loss: 0.0167 - acc: 0.9776\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 97us/step - loss: 0.0164 - acc: 0.9776\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 89us/step - loss: 0.0163 - acc: 0.9776\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 93us/step - loss: 0.0163 - acc: 0.9776\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 93us/step - loss: 0.0163 - acc: 0.9776\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 92us/step - loss: 0.0168 - acc: 0.9774\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 93us/step - loss: 0.0169 - acc: 0.9776\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 93us/step - loss: 0.0173 - acc: 0.9772\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 91us/step - loss: 0.0169 - acc: 0.9774\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 93us/step - loss: 0.0171 - acc: 0.9776\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 92us/step - loss: 0.0172 - acc: 0.9774\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 88us/step - loss: 0.0186 - acc: 0.9765\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 93us/step - loss: 0.0249 - acc: 0.9730\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 93us/step - loss: 0.0199 - acc: 0.9756\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 102us/step - loss: 0.0197 - acc: 0.9763\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 96us/step - loss: 0.0184 - acc: 0.9767\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 91us/step - loss: 0.0170 - acc: 0.9772\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 93us/step - loss: 0.0166 - acc: 0.9772\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 90us/step - loss: 0.0165 - acc: 0.9774\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 87us/step - loss: 0.0163 - acc: 0.9774\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 99us/step - loss: 0.0160 - acc: 0.9772\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 87us/step - loss: 0.0164 - acc: 0.9774\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 96us/step - loss: 0.0189 - acc: 0.9772\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 87us/step - loss: 0.0168 - acc: 0.9774\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 87us/step - loss: 0.0160 - acc: 0.9776\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 88us/step - loss: 0.0163 - acc: 0.9779\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 102us/step - loss: 0.0156 - acc: 0.9781\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 108us/step - loss: 0.0154 - acc: 0.9781\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 101us/step - loss: 0.0153 - acc: 0.9781\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 110us/step - loss: 0.0153 - acc: 0.9781\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 95us/step - loss: 0.0153 - acc: 0.9781\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 104us/step - loss: 0.0153 - acc: 0.9781\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 104us/step - loss: 0.0153 - acc: 0.9781\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 92us/step - loss: 0.0151 - acc: 0.9781\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 102us/step - loss: 0.0152 - acc: 0.9781\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 94us/step - loss: 0.0153 - acc: 0.9781\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 94us/step - loss: 0.0160 - acc: 0.9779\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 91us/step - loss: 0.0161 - acc: 0.9774\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 93us/step - loss: 0.0160 - acc: 0.9779\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 88us/step - loss: 0.0159 - acc: 0.9779\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 88us/step - loss: 0.0166 - acc: 0.9776\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 94us/step - loss: 0.0170 - acc: 0.9774\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 107us/step - loss: 0.0160 - acc: 0.9781\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 89us/step - loss: 0.0155 - acc: 0.9781\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 88us/step - loss: 0.0157 - acc: 0.9779\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 88us/step - loss: 0.0158 - acc: 0.9781\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 94us/step - loss: 0.0159 - acc: 0.9776\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 89us/step - loss: 0.0160 - acc: 0.9779\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 89us/step - loss: 0.0157 - acc: 0.9781\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 87us/step - loss: 0.0159 - acc: 0.9779\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 91us/step - loss: 0.0169 - acc: 0.9779\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 91us/step - loss: 0.0164 - acc: 0.9776\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 94us/step - loss: 0.0158 - acc: 0.9781\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 94us/step - loss: 0.0156 - acc: 0.9781\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 94us/step - loss: 0.0153 - acc: 0.9781\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 93us/step - loss: 0.0151 - acc: 0.9783\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 91us/step - loss: 0.0150 - acc: 0.9783\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 91us/step - loss: 0.0149 - acc: 0.9783\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 98us/step - loss: 0.0150 - acc: 0.9783\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 93us/step - loss: 0.0149 - acc: 0.9783\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 90us/step - loss: 0.0149 - acc: 0.9783\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 91us/step - loss: 0.0149 - acc: 0.9783\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 92us/step - loss: 0.0149 - acc: 0.9783\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 90us/step - loss: 0.0149 - acc: 0.9783\n",
      "Epoch 1/1\n",
      "4337/4337 [==============================] - 0s 92us/step - loss: 0.0148 - acc: 0.9783\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    model.fit(scaled_fingerprints, training_labels, batch_size=32, epochs=1)\n",
    "    predictions = model.predict(scaled_val_fingerprints)\n",
    "    auc = roc_auc_score(val_labels, predictions, average=\"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9958941951108756"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc\n",
    "## OUTPUT: 0.9958941951108756"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous model we use ....,.... and got a $0.99589$ auc score, which is relatively a good score. However, we'd still need to see what parameters play the biggest role in order for this model to learn. i.e. what parameters have biggest influence on updating the weights of this model. By doing so, we are also able to understand more about the data.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Second Model\n",
    "\n",
    "Let's take a look on what things that are featurign this model. This model has ... learning rate instead of .... and instead .. and unline in the previous one it has ... instead .... . So how does this model comapre to the previous one in terms of accuracy, prediction, and auc score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a table that shows the comparison of all models. Learning rate, model architecture and and wether or not Dropout was used, are all depicted for each model:\n",
    "\n",
    "|Model_nr|Learning Rate|Dropout|Nom of layers |Activation Functions|       OutputLayer        |\n",
    "|--------|-------------|-------|--------------|--------------------|--------------------------|\n",
    "|1       |0.01         |no     |1 hidden layer|relu                |one-dimensional output dense with linear activation|\n",
    "|2       |\n",
    "\n",
    "Additionally, you can refer to the model's summary by ty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "<div style=\"text-align: center;\"> <b>Please ingore everything below this here<b>\n",
    "    \n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    Ù§\n",
    "</div>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(16, input_shape=(4337, 2048,), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_43 (Dense)             (None, 4337, 16)          32784     \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 4337, 32)          544       \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 4337, 2)           66        \n",
      "=================================================================\n",
      "Total params: 33,394\n",
      "Trainable params: 33,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 1s - loss: 0.2355\n",
      "Epoch 2/20\n",
      " - 1s - loss: 0.2149\n",
      "Epoch 3/20\n",
      " - 1s - loss: 0.1989\n",
      "Epoch 4/20\n",
      " - 1s - loss: 0.1866\n",
      "Epoch 5/20\n",
      " - 1s - loss: 0.1763\n",
      "Epoch 6/20\n",
      " - 1s - loss: 0.1674\n",
      "Epoch 7/20\n",
      " - 1s - loss: 0.1594\n",
      "Epoch 8/20\n",
      " - 1s - loss: 0.1524\n",
      "Epoch 9/20\n",
      " - 1s - loss: 0.1460\n",
      "Epoch 10/20\n",
      " - 1s - loss: 0.1406\n",
      "Epoch 11/20\n",
      " - 1s - loss: 0.1357\n",
      "Epoch 12/20\n",
      " - 1s - loss: 0.1312\n",
      "Epoch 13/20\n",
      " - 1s - loss: 0.1271\n",
      "Epoch 14/20\n",
      " - 1s - loss: 0.1234\n",
      "Epoch 15/20\n",
      " - 1s - loss: 0.1199\n",
      "Epoch 16/20\n",
      " - 1s - loss: 0.1166\n",
      "Epoch 17/20\n",
      " - 1s - loss: 0.1137\n",
      "Epoch 18/20\n",
      " - 1s - loss: 0.1106\n",
      "Epoch 19/20\n",
      " - 1s - loss: 0.1081\n",
      "Epoch 20/20\n",
      " - 1s - loss: 0.1055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a52a866a0>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(fingerprints, training_labels, validation_data=val_set.all(), batch_size=20, epochs=20, shuffle=True, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
