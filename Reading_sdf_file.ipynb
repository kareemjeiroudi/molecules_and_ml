{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Notes\n",
    "\n",
    "Please do not run the file as whole, as some cells include some irrelevant code that was created for testing some examples. The cells to be ignored are noted by a **BEGIN:** and **END;** notations.\n",
    "\n",
    "First, let's go through the imports we'll need for this session. Iâ€™ll need RDkit for molecular conversion and fingerprints calculation, numpy for and data management and numpy arrays, Scikit-learn for standardisation and data set splitting as well as Keras for the neural network building and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data\n",
    "\n",
    "Reading a set of molecules an `SDMolSupplier` from rdkit.<br>\n",
    "In the [getting started](https://www.rdkit.org/docs/GettingStartedInPython.html) documentation there're two these two different suppliers:\n",
    "* `rdkit.Chem.rdmolfiles.SDMolSupplier`\n",
    "* `rdkit.Chem.rdmolfiles.SmilesMolSupplier`\n",
    "\n",
    "We're gonna be using the `SDMolSupplier`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4337"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read the file\n",
    "supplier = Chem.SDMolSupplier('data/cas_4337.sdf')\n",
    "len(supplier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4337 molecules in the data set:\n",
    "A good practice is to test each molecule to see if it was correctly read before working with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mol in supplier:\n",
    "    if mol is None:\n",
    "        print(\"a None molecule was found!\")\n",
    "        \n",
    "## if the output is empty the data is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Morgan fingerprints\n",
    "\n",
    "We're going to go trough two steps:<br>\n",
    "\n",
    "First, we'll obtain the training samples, which are going to be the bits value returned by the function `AllChem.GetMorganFingerprintAsBitVect`.<br>This is typically the output of calculating the morgan fingerprints for each molecule. Please refer to the subsection '**Explaining bits from Morgan Fingerprints**' to understand the output.<br>\n",
    "After that we'd only need to turn it to a numpy array, so that we can pass it to Keras's sequential model. Keras's model expects the training set to be either a numpy array (if the model has a single input), or list of Numpy arrays (if the model has multiple inputs).\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "> **\\_\\_TASK\\_\\_:** for each molecule calculate MorganFingerprints (with radius <b>3</b>) and size **~2048** (rdkit has also a nice easy function for that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rdkit.Chem import AllChem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4337, 2048)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = {} # will be mutated in the next function\n",
    "## calculate the Morgan Fingerprints for every molecule in the supplier\n",
    "fingerprints = [AllChem.GetMorganFingerprintAsBitVect(mol, 3, nBits=2048, bitInfo=info) for mol in supplier]\n",
    "## convert it from bit vector to NumPy array\n",
    "fingerprints = np.array(fingerprints)\n",
    "\n",
    "fingerprints.shape\n",
    "# OUTPUT: (4337, 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `bitsInfo` - Explaining bits from Morgan Fingerprints\n",
    "\n",
    "> **\\_\\_TASK\\_\\_:** Important! when you calculate the Fingerprints, save which atoms where responsible for the activation of the fingerprint (rdkit can also do that)\n",
    "\n",
    "Information about which atoms are contributing to the activation of the fingerprint is stored in the dictionary `bitsInfo`. The dictionary provided is populated with one entry per bit set in the fingerprint, <u>the keys are the bit ids</u>, <u>the values are lists of (atom index, radius) tuples</u>. Let's have a quick look on the dictionary in `info`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{97: ((14, 0),),\n",
       " 191: ((10, 1),),\n",
       " 263: ((4, 2),),\n",
       " 314: ((7, 1), (9, 1)),\n",
       " 325: ((15, 3),),\n",
       " 336: ((2, 2), (3, 2)),\n",
       " 389: ((17, 2),),\n",
       " 484: ((6, 3), (8, 3)),\n",
       " 606: ((11, 2),),\n",
       " 650: ((5, 0), (7, 0), (9, 0)),\n",
       " 689: ((1, 1),),\n",
       " 703: ((12, 2), (13, 2)),\n",
       " 807: ((1, 0),),\n",
       " 811: ((6, 2), (8, 2)),\n",
       " 856: ((11, 3),),\n",
       " 905: ((0, 3),),\n",
       " 993: ((0, 2),),\n",
       " 1019: ((0, 0),),\n",
       " 1034: ((2, 1), (3, 1)),\n",
       " 1060: ((6, 1), (8, 1)),\n",
       " 1077: ((10, 2),),\n",
       " 1088: ((15, 1), (16, 1), (17, 1)),\n",
       " 1114: ((6, 0), (8, 0)),\n",
       " 1152: ((4, 0),),\n",
       " 1199: ((15, 2), (16, 2)),\n",
       " 1216: ((1, 3),),\n",
       " 1327: ((10, 3),),\n",
       " 1380: ((2, 0), (3, 0), (10, 0), (11, 0)),\n",
       " 1460: ((12, 3),),\n",
       " 1642: ((4, 3),),\n",
       " 1645: ((11, 1),),\n",
       " 1682: ((1, 2),),\n",
       " 1717: ((14, 1),),\n",
       " 1750: ((12, 1), (13, 1)),\n",
       " 1771: ((0, 1),),\n",
       " 1816: ((4, 1),),\n",
       " 1873: ((12, 0), (13, 0), (15, 0), (16, 0), (17, 0)),\n",
       " 1917: ((5, 1),),\n",
       " 1947: ((17, 3),),\n",
       " 2037: ((2, 3),)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Interpreting the results above:\n",
    "\n",
    "```\n",
    "97: ((14, 0),),\n",
    "191: ((10, 1),),\n",
    "314: ((7, 1), (9, 1)),\n",
    "```\n",
    "\n",
    "* bit 97 is set once: by atom 14 with radius 0.\n",
    "* bit 191 is set also once: by atom 10, with radius 1.\n",
    "* bit 314, on the other hand, is set twice: once by atom 7 and once with atom 9, each with radius 1.\n",
    "\n",
    "<br>\n",
    "\n",
    "And there we go. Now have succefully extracted the training samples that are ready to be passed to the model.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Training And Validation Samples\n",
    "\n",
    "Second, I'd like to take a moment to split the samples we have into training samples and validation samples. It's important to note that the model is not supposed to learn on the validation samples (In another word, samples in the validation set are not supposed to appear in the training set), therefore we'd need to subset the fingerprints NumPy array at hand.\n",
    "\n",
    "For that purpose, we have randomly picked the following indices of molecules.<br>\n",
    "*In the following, I'll be using the alias `valid_`, for anything that's related to the validation set, in order to avoid lengthy variable names.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KAREEM: these molecules I got from Kristina!\n",
    "val_ids = [6,   10,   29,   32,   42,   58,   72,   83,   98,  100,  128, \n",
    "        145,  148,  168,  171,  205,  208,  237,  244,  285,  290,  291,\n",
    "         300,  312,  332,  334,  335,  347,  356,  369,  371,  377,  407,\n",
    "         424,  456,  458,  470,  472,  486,  514,  515,  528,  557,  563,\n",
    "         599,  610,  616,  628,  640,  701,  704,  722,  764,  794,  818,\n",
    "         821,  840,  850,  856,  859,  874,  878,  882,  898,  901,  925,\n",
    "         936,  945,  957,  974,  977, 1013, 1019, 1030, 1038, 1047, 1049,\n",
    "        1072, 1073, 1100, 1159, 1168, 1187, 1190, 1194, 1201, 1202, 1233,\n",
    "        1247, 1258, 1264, 1273, 1283, 1288, 1300, 1302, 1319, 1339, 1349,\n",
    "        1402, 1413, 1416, 1422, 1426, 1435, 1454, 1465, 1483, 1502, 1513,\n",
    "        1515, 1520, 1548, 1576, 1604, 1606, 1621, 1650, 1695, 1696, 1711,\n",
    "        1714, 1716, 1725, 1743, 1746, 1752, 1780, 1788, 1794, 1799, 1813,\n",
    "        1826, 1866, 1886, 1901, 1903, 1921, 1929, 1940, 1969, 1970, 1997,\n",
    "        1998, 2008, 2010, 2011, 2018, 2023, 2046, 2060, 2064, 2080, 2081,\n",
    "        2131, 2171, 2182, 2203, 2212, 2224, 2231, 2241, 2246, 2283, 2294,\n",
    "        2295, 2297, 2327, 2329, 2331, 2349, 2357, 2360, 2365, 2397, 2413,\n",
    "        2417, 2418, 2421, 2448, 2467, 2510, 2516, 2528, 2533, 2549, 2562,\n",
    "        2601, 2604, 2606, 2609, 2611, 2632, 2644, 2653, 2677, 2682, 2685,\n",
    "        2692, 2703, 2708, 2714, 2719, 2726, 2732, 2759, 2761, 2776, 2780,\n",
    "        2817, 2818, 2829, 2837, 2857, 2858, 2884, 2899, 2902, 2905, 2911,\n",
    "        2939, 2975, 2977, 2986, 3007, 3009, 3018, 3024, 3038, 3066, 3087,\n",
    "        3098, 3107, 3117, 3122, 3139, 3157, 3161, 3164, 3217, 3223, 3233,\n",
    "        3263, 3265, 3271, 3290, 3295, 3307, 3313, 3317, 3321, 3382, 3384,\n",
    "        3388, 3400, 3409, 3412, 3419, 3423, 3449, 3470, 3487, 3488, 3503,\n",
    "        3509, 3511, 3539, 3562, 3626, 3637, 3654, 3662, 3663, 3668, 3671,\n",
    "        3688, 3689, 3695, 3710, 3726, 3743, 3744, 3782, 3791, 3794, 3808,\n",
    "        3809, 3841, 3849, 3874, 3910, 3912, 3925, 3945, 3950, 3958, 3959,\n",
    "        3962, 3964, 3967, 3978, 3993, 4009, 4010, 4055, 4057, 4085, 4089,\n",
    "        4096, 4099, 4107, 4112, 4129, 4135, 4151, 4155, 4196, 4209, 4216,\n",
    "        4234, 4236, 4251, 4267, 4283, 4317, 4326, 4335\n",
    "\n",
    "]\n",
    "\n",
    "train_samples = []\n",
    "valid_samples = []\n",
    "for i in range(len(fingerprints)):\n",
    "    if i in val_ids:\n",
    "        valid_samples.append(fingerprints[i])\n",
    "    else:\n",
    "        train_samples.append(fingerprints[i])\n",
    "train_samples = np.array(train_samples)\n",
    "valid_samples = np.array(valid_samples)\n",
    "\n",
    "\n",
    "# train_samples.shape\n",
    "# (4010, 2048)\n",
    "# valid_samples.shape\n",
    "# (327, 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training And Validation Labels\n",
    "\n",
    "Now that we've got the the bits for both the training samples as well as for the validation samples ready, we still have one last thing to do before we start building the model: extract the labels to have a target for our predictions.\n",
    "\n",
    "The property that we want to extract out of the list, is whether or not the 'Ames test Categorisation' is a mutagen.\n",
    "\n",
    "* 1 for mutagen \n",
    "* 0 for nonmutagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "for mol in supplier:\n",
    "    if mol.GetProp(\"Ames test categorisation\") == \"mutagen\":\n",
    "        targets.append(1)\n",
    "    else:\n",
    "        targets.append(0)\n",
    "        \n",
    "# len(targets)\n",
    "# 4337\n",
    "\n",
    "train_labels = []\n",
    "valid_labels = []\n",
    "for i in range(len(targets)):\n",
    "    if i in val_ids:\n",
    "        valid_labels.append(targets[i])\n",
    "    else:\n",
    "        train_labels.append(targets[i])\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "valid_labels = np.array(valid_labels)\n",
    "\n",
    "# train_labels.shape\n",
    "# (4010,)\n",
    "# valid_labels.shape\n",
    "# (327,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaling\n",
    "\n",
    "Finally we have everything ready to pass to the model.\n",
    "However, the last step in preparing data is to apply Standard Scaling, because the model cannot learn on some arbitrary numbers. Additionally, I'd need to scale down the validation samples too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#Scale fingerprints to unit variance and zero mean\n",
    "st = StandardScaler()\n",
    "train_samples = st.fit_transform(train_samples)\n",
    "valid_samples = st.transform(valid_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model's Architecture\n",
    "\n",
    "From before we know that our data shape is `(4337, 2048)`, which means that the first layer in our model expects an input size of $2048$.<br>\n",
    "The following layers, on the other hand, can deduce their input size from the previous layer. For instance, the output size of the first layer in my model is 5, beginning with the second layer, I don't need to specify the input size, because the layer can deduce that alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to suppress the FutureWarning: conversion of the second argument of issubdtype \n",
    "# from 'float' to 'np.floatin' is deprecated\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "# importing all libraries that we'd need\n",
    "from keras import backend\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score # Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores\n",
    "\n",
    "## for Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell, in case you don't want to recalculate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = np.loadtxt(\"data/train_samples.txt\", delimiter=' ', comments='# ', encoding=None)\n",
    "train_labels = np.loadtxt(\"data/train_labels.txt\", delimiter=' ', comments='# ', encoding=None)\n",
    "valid_samples = np.loadtxt(\"data/valid_samples.txt\", delimiter=' ', comments='# ', encoding=None)\n",
    "valid_labels = np.loadtxt(\"data/valid_labels.txt\", delimiter=' ', comments='# ', encoding=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend.clear_session()\n",
    "model = Sequential()\n",
    "initializer = 'lecun_uniform'\n",
    "model.add(Dense(256, input_dim=train_samples.shape[1], activation='relu',\n",
    "               kernel_initializer=initializer))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=int(256/8), activation='hard_sigmoid',\n",
    "                kernel_initializer=initializer))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I used the fasted learning rate possible because running 500 epochs takes relatively long time $(\\approx1.5\\,min)$ on my 2017 machine. For optimizers, I used standard SGD, which <...>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True), metrics=[\"accuracy\"])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adadelta(lr=0.1), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 532,801\n",
      "Trainable params: 532,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for us to predict the <>, we used the function `sklearn.metrics.roc_auc_score`, which computes Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "Note: this implementation is restricted to the binary classification task or multilabel classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The recommendation of Kristina:<br>\n",
    "Use one `fit` and one `predict` at a time and instead of running multiple epochs, you can use a for loop in order for you validate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 9s 2ms/step - loss: 0.6755 - acc: 0.5761\n",
      "0.7917959585256944\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.5891 - acc: 0.6858\n",
      "0.8179822901687732\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.5316 - acc: 0.7434\n",
      "0.8376977219405132\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.4761 - acc: 0.7875\n",
      "0.8542344660561568\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.4289 - acc: 0.8102\n",
      "0.8661923862862332\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.3891 - acc: 0.8364\n",
      "0.875728449254522\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.3563 - acc: 0.8574\n",
      "0.8835616438356164\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.3285 - acc: 0.8713\n",
      "0.8878755770831757\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.2975 - acc: 0.8825\n",
      "0.8923408764095966\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.2712 - acc: 0.8988\n",
      "0.894308635434799\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 9s 2ms/step - loss: 0.2464 - acc: 0.9085\n",
      "0.8964656020585786\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 9s 2ms/step - loss: 0.2286 - acc: 0.9165\n",
      "0.8965034435782941\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.2117 - acc: 0.9279\n",
      "0.8977900552486188\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 9s 2ms/step - loss: 0.1925 - acc: 0.9334\n",
      "0.8975630061303261\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 9s 2ms/step - loss: 0.1744 - acc: 0.9384\n",
      "0.8970332248543101\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 9s 2ms/step - loss: 0.1596 - acc: 0.9479\n",
      "0.8975251646106108\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 9s 2ms/step - loss: 0.1482 - acc: 0.9536\n",
      "0.8988874593203664\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.1381 - acc: 0.9554\n",
      "0.8974873230908953\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.1231 - acc: 0.9611\n",
      "0.8981306289260577\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 9s 2ms/step - loss: 0.1144 - acc: 0.9641\n",
      "0.8983198365246349\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.1062 - acc: 0.9663\n",
      "0.8979603420873382\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 9s 2ms/step - loss: 0.0974 - acc: 0.9698\n",
      "0.8980927874063422\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 9s 2ms/step - loss: 0.0878 - acc: 0.9763\n",
      "0.8984333610837811\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.0806 - acc: 0.9781\n",
      "0.8979792628471959\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.0746 - acc: 0.9796\n",
      "0.8978657382880497\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.0702 - acc: 0.9800\n",
      "0.8975251646106108\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.0645 - acc: 0.9828\n",
      "0.8975251646106108\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.0574 - acc: 0.9843\n",
      "0.8977522137289033\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 9s 2ms/step - loss: 0.0559 - acc: 0.9833\n",
      "0.8970710663740256\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 9s 2ms/step - loss: 0.0495 - acc: 0.9873\n",
      "0.8966548096571557\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.0484 - acc: 0.9878\n",
      "0.8962763944600015\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.0442 - acc: 0.9875\n",
      "0.8968061757360175\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.0431 - acc: 0.9895\n",
      "0.896844017255733\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 9s 2ms/step - loss: 0.0343 - acc: 0.9920\n",
      "0.8968061757360175\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.0368 - acc: 0.9913\n",
      "0.8966548096571559\n",
      "Epoch 1/1\n",
      "4010/4010 [==============================] - 8s 2ms/step - loss: 0.0341 - acc: 0.9918\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    model.fit(train_samples, train_labels, batch_size=20, epochs=1)\n",
    "    predictions = model.predict(valid_samples)\n",
    "    auc = roc_auc_score(valid_labels, predictions)\n",
    "    print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc\n",
    "## OUTPUT:0.8720956633618406"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous model we use ....,.... and got a $0.743$ auc score, which is relatively a good score. However, we'd still need to see what parameters play the biggest role in order for this model to learn. i.e. what parameters have biggest influence on updating the weights of this model. By doing so, we are also able to understand more about the data.\n",
    "\n",
    "<br>\n",
    "\n",
    "# Prediction Analysis\n",
    "\n",
    "Let's now together examine the success of this model's predictions.<br>\n",
    "Because the type of classificaiton is *binary*. i.e. the output is either $0$ or $1$ ('nonmutagen' or 'mutagen'), unlike in *multilabel classification* where the output can any float value ranging from $0$ to $1$, we can use a confusion matrix which shows true postives with respect to false postives, and true negatives with respect to false negatives - more on that in the following [**remark on Confusion Matrix**](Reading_sdf_file.ipynb#Remark-on-Confusion-Matrix).<br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(valid_labels, model.predict_classes(valid_samples))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cm_plot_labels = ['nonmutagen', 'mutagen']\n",
    "plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark on Confusion Matrix:\n",
    "\n",
    "Apparantly, the model has a lower chance of predicting nonmutagens than mutagens, indicated by the darker blue color on the plot above. Tweaking the parameters later, will help us find which input from bits has highest significance during prediction.\n",
    "\n",
    "Interpreting the results above:\n",
    "* In $64$ out of $181$ cases where the true label was '**mutagen**', the model predicted that it was '**nonmutagen**'.\n",
    "* In $40$ cases the model predited it to be '**mutagen**', where the true label was '**nonmutagen**',.\n",
    "* The rest of predictions were accurate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters Optimization\n",
    "\n",
    "Neural Networks are notoriously difficult to configure and there are a lot of parameters that need to be set. Above all this, it's a very slow process to train and evaulate every model you build. Therefore, one can use optimization methods that allow faster hyperparameters substitution. Using these methods, we can acheive an automated optimization process.<br>\n",
    "Specifically, I'll be using the grid search method, since Keras library has a plenty of grid-search capable functions.\n",
    "\n",
    "Keras models can be used by wrapping them with the KerasClassifier. To use these wrappers we must define a function that creates and returns your Keras sequential model, then pass this function to the `build_fn` argument when constructing the KerasClassifier class.<br>\n",
    "The constructor for the KerasClassifier class can take default arguments that are passed on to the calls to `model.fit()`, such as the number of epochs and the batch size.<br>\n",
    "The constructor for the KerasClassifier class can also take newly specified arguments that can be passed to your custom `create_model()` function. These new arguments must also be defined in the signature of your `create_model()` function with default parameters.\n",
    "\n",
    "We use average accuracy as an estimate for out-of-sample accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use scikit-learn to grid search the batch size and epochs\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learn_rate=0.01, momentum=0, init_mode='uniform', activation='relu',\n",
    "                dropout_rate=0.0, weight_constraint=0, neurons=1):\n",
    "    '''Function to create model, required for KerasClassifier'''\n",
    "    ## create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=8, kernel_initializer=init_mode, activation=activation,\n",
    "                       kernel_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    "    ## Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4007b1704a67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# -1 to run in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;31m# Regenerate parameter iterable for each fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0mcandidate_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_param_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m         \u001b[0mn_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "## define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', \n",
    "             'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh',\n",
    "              'sigmoid', 'hard_sigmoid']\n",
    "weight_constraint = [1, 2, 3, 4, 5]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "neurons = [5, 10, 20, 50, 100, 200, 256, 512, 2048]\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, learn_rate=learn_rate, momentum=momentum,\n",
    "                 init_mode=init_mode, activation=activation, dropout_rate=dropout_rate,\n",
    "                  weight_constraint=weight_constraint, neurons=neurons)\n",
    "\n",
    "## create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "## time the iterations\n",
    "from datetime import datetime\n",
    "start=datetime.now()\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1) # -1 to run in parallel\n",
    "grid_result = grid.fit(valid_samples, valid_labels)\n",
    "print(datetime.now()-start)\n",
    "\n",
    "## summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is just in order for me to transfer the bits data to another machine, without having to recalculate them.<br>\n",
    "The cell is saved in raw format, because I've already run the code. If you'd like to overwrite the files, rerun the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save data for easy reproduction\n",
    "np.savetxt(\"data/train_samples.txt\", train_samples , delimiter=' ', comments='# ', encoding=None)\n",
    "np.savetxt(\"data/train_labels.txt\", train_labels , delimiter=' ', comments='# ', encoding=None)\n",
    "np.savetxt(\"data/valid_samples.txt\", valid_samples , delimiter=' ', comments='# ', encoding=None)\n",
    "np.savetxt(\"data/valid_labels.txt\", valid_labels , delimiter=' ', comments='# ', encoding=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_whatever == train_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "<div style=\"text-align: center;\"> <b>Please ingore everything below this here<b>\n",
    "    \n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    |\n",
    "    Ù§\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Second Model\n",
    "\n",
    "Let's take a look on what things that are featurign this model. This model has ... learning rate instead of .... and instead .. and unline in the previous one it has ... instead .... . So how does this model comapre to the previous one in terms of accuracy, prediction, and auc score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a table that shows the comparison of all models. Learning rate, model architecture and and wether or not Dropout was used, are all depicted for each model:\n",
    "\n",
    "|Model_nr|Learning Rate|Dropout|Nom of layers |Activation Functions|       OutputLayer        |\n",
    "|--------|-------------|-------|--------------|--------------------|--------------------------|\n",
    "|1       |0.01         |no     |1 hidden layer|relu                |one-dimensional output dense with linear activation|\n",
    "|2       |\n",
    "\n",
    "Additionally, you can refer to the model's summary by ty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(16, input_shape=(4337, 2048,), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(fingerprints, training_labels, validation_data=val_set.all(), batch_size=20, epochs=20, shuffle=True, verbose=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
